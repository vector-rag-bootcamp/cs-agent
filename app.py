import os
from dotenv import load_dotenv
load_dotenv()

OPENAI_API_KEY = os.environ.get("OPEN_API_KEY")
GENAI_BASE_URL = os.environ.get("OPENAI_BASE_URL")

import streamlit as st

from utils.database import *
from utils.etc import format_docs, stream_data

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


st.markdown(
    """
    <style>
    [data-testid="stChatMessageAvatarUser"] {
        background-color: #28cdc8 !important;
    }
    [data-testid="stChatMessageAvatarAssistant"] {
        background-color: #ffe65a !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)


st.sidebar.checkbox(":globe_with_meridians: web ê²€ìƒ‰", value=False, key="bar")

st.session_state["genai_model_name"] = "Meta-Llama-3.1-8B-Instruct"
st.session_state["embed_model_name"] = "BAAI/bge-m3"


# Set LLM
llm = ChatOpenAI(
    model=st.session_state.genai_model_name,
    temperature=0,
    max_tokens=None,
    base_url=GENAI_BASE_URL,
    api_key=OPENAI_API_KEY
)


# Set Vectorstore and Retriever
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = load_vectorstore(load_path="./db/vdb_0218-0936", isWeb=st.session_state.bar, embedding_model=st.session_state.embed_model_name)
    st.session_state.retriver = st.session_state.vectorstore.as_retriever(search_kwargs={"k": 5}) # search_type="mmr": ê´€ë ¨ì„±/ë‹¤ì–‘ì„± ê³ ë ¤ ë¹„ìœ¨ ì„¤ì • ê°€ëŠ¥
    
retriever = st.session_state.retriver


# Set Prompt
template = """
ë‹¤ìŒ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ì„ ìƒì„±í•˜ê¸° ì „ì— ë‹¤ìŒì„ ë‚´ë¶€ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì„¸ìš”:
1. ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ ìš”êµ¬ ì‚¬í•­ì„ íŒŒì•…í•©ë‹ˆë‹¤.
2. ë¬¸ë§¥ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.
3. ë…¼ë¦¬ì  ì‚¬ê³ ë¥¼ í†µí•´ ê°€ì¥ ì í•©í•œ ë‹µë³€ì„ ë„ì¶œí•©ë‹ˆë‹¤.

ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ëª…í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€ë§Œ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€:
"""
prompt_template = ChatPromptTemplate.from_template(template)


# Configure Chain
chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt_template
    | llm
    | StrOutputParser()
)


# 1. Chat Page
def chat(): # st.Pageì˜ ì²«ë²ˆì¬ parameterëŠ” "~.py" íŒŒì´ì¬ íŒŒì¼ ëª… ë˜ëŠ” í•¨ìˆ˜ëª…
    st.info("""ì•ˆë…•í•˜ì„¸ìš”. AI ê³ ê° ì§€ì› ì„¼í„°ì…ë‹ˆë‹¤.\n\në¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? :sunglasses:""", icon="â„¹ï¸")
    st.divider()

    # Accept user input
    if prompt := st.chat_input("Type a message..."):
        # Display user message in chat message container
        with st.chat_message("user"):
            st.markdown(prompt)
        
               
        # Invoke RAG chain
        with st.chat_message("assistant"):
            with st.spinner('Waiting...'):
                try:   
#                     st.write(retriever.invoke(prompt)) # DEBUG: retriever works
#                     st.write(f"LLM reponse: {llm.invoke(prompt)}") # DEBUG: Internal Server Error
                    full_response = chain.invoke(prompt)

                    st.write_stream(stream_data(full_response))

                except Exception as e:
                    # Handle potential errors
                    st.write(f"Error processing the request: {e}")


# 2. Settings Page
def settings():
    st.warning("ì•ˆë…•í•˜ì„¸ìš”. ì´ê³³ì€ ì„¤ì •í˜ì´ì§€ì…ë‹ˆë‹¤.", icon="âš™ï¸")
    st.divider()
    
    st.subheader(":mag: RAG êµ¬ì„±")
    st.write(f"- Generator: {st.session_state.genai_model_name}\n- Embedding: {st.session_state.embed_model_name}\n- Retriver: FAISS")
    
    st.subheader(f":globe_with_meridians: Web ê²€ìƒ‰")
    st.write("- ON" if st.session_state.bar else "- OFF")



pages = [st.Page(chat, title="chat", icon="ğŸ’¬"), st.Page(settings, title="settings", icon="âš™ï¸")]
pg = st.navigation(pages)
pg.run()