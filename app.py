import os
from dotenv import load_dotenv
load_dotenv()

OPENAI_API_KEY = os.environ.get("OPEN_API_KEY")
GENAI_BASE_URL = os.environ.get("OPENAI_BASE_URL")

import streamlit as st

from utils.database import *
from utils.etc import format_docs, stream_data

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


st.markdown(
    """
    <style>
    [data-testid="stChatMessageAvatarUser"] {
        background-color: #28cdc8 !important;
    }
    [data-testid="stChatMessageAvatarAssistant"] {
        background-color: #ffe65a !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)


st.session_state["genai_model_name"] = "Meta-Llama-3.1-8B-Instruct"
st.session_state["embed_model_name"] = "BAAI/bge-multilingual-gemma2"


# Set LLM
llm = ChatOpenAI(
    model=st.session_state.genai_model_name,
    temperature=0,
    max_tokens=None,
    base_url=GENAI_BASE_URL,
    api_key=OPENAI_API_KEY
)

# Set Prompt
template = """
ë‹¤ìŒ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì‚¬ê³  ê³¼ì •ì„ í†µí•´ ìƒì„±í•˜ì„¸ìš”:
1. ì§ˆë¬¸ì„ ë¶„ì„í•©ë‹ˆë‹¤.
2. ë¬¸ë§¥ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.
3. ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ ë…¼ë¦¬ì  ë‹¨ê³„ë¥¼ ë‚˜ì—´í•©ë‹ˆë‹¤.
4. ìµœì¢…ì ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•˜ê³  ê°„ê²°í•œ ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.

ë‹µë³€:
"""

prompt_template = ChatPromptTemplate.from_template(template)

retriever = st.session_state.retriver

chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt_template
    | llm
    | StrOutputParser()
)


# 1. Chat Page
def chat(): # st.Pageì˜ ì²«ë²ˆì¬ parameterëŠ” "~.py" íŒŒì´ì¬ íŒŒì¼ ëª… ë˜ëŠ” í•¨ìˆ˜ëª…
    # st.title("AI ê³ ê° ì§€ì› ì„¼í„°")
    st.info("""ì•ˆë…•í•˜ì„¸ìš”. AI ê³ ê° ì§€ì› ì„¼í„°ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? :sunglasses:""", icon="â„¹ï¸")
    st.divider()

    # Accept user input
    if prompt := st.chat_input("Type a message..."):
        # Display user message in chat message container
        with st.chat_message("user"):
            st.markdown(prompt)
        
        try:
            # Invoke RAG chain
            with st.chat_message("assistant"):
                with st.spinner('Waiting...'):
                    full_response = chain.invoke(prompt)
                
                st.write_stream(stream_data(full_response))

        except Exception as e:
            # Handle potential errors
            st.write(f"Error processing the request: {e}")


# 2. Settings Page
def settings():
    # st.title("AI ê³ ê° ì§€ì› ì„¼í„°: ì„¤ì •")
    st.warning("ì•ˆë…•í•˜ì„¸ìš”. ì´ê³³ì€ ì„¤ì •í˜ì´ì§€ì…ë‹ˆë‹¤.", icon="âš™ï¸")
    st.divider()
    
    st.subheader(":mag: RAG êµ¬ì„±")
    st.write(f"- Generator: {st.session_state.genai_model_name}\n- Embedding: {st.session_state.embed_model_name}\n- Retriver: FAISS")
    
    st.subheader(f":globe_with_meridians: Web ê²€ìƒ‰")
    st.write("- ON" if st.session_state.bar else "- OFF")


# Widgets shared by all the pages
st.sidebar.checkbox(":globe_with_meridians: web ê²€ìƒ‰", [True, False], key="bar")


pages = [st.Page(chat, title="chat", icon="ğŸ’¬"), st.Page(settings, title="settings", icon="âš™ï¸")]
pg = st.navigation(pages)
pg.run()