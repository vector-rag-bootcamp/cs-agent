{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imperial-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intimate-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "collect-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add root folder of the rag_bootcamp repo to PYTHONPATH\n",
    "current_dir = Path().resolve()\n",
    "parent_dir = current_dir.parent\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "from utils.load_secrets import load_env_file\n",
    "load_env_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "banned-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_BASE_URL = os.environ.get(\"OPENAI_BASE_URL\")\n",
    "OPEN_API_KEY = os.environ.get(\"OPEN_API_KEY\")\n",
    "\n",
    "GENERATOR_MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "EMBEDDING_MODEL_NAME = \"BAAI/bge-base-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "muslim-landscape",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the embeddings model...\n"
     ]
    }
   ],
   "source": [
    "# Load docs\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split (make chunks)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "model_kwargs = {'device': 'cuda', 'trust_remote_code': True}\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "# Set embeddings model\n",
    "print(f\"Setting up the embeddings model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "\n",
    "# Store splits(=chunks)\n",
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "peripheral-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(model=GENERATOR_MODEL_NAME,  # 사용할 모델 이름\n",
    "                 temperature=0,\n",
    "                 base_url=GENERATOR_BASE_URL,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abandoned-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 프롬프트 템플릿 정의 (LangChain Hub의 템플릿을 로컬에서 구현): docs에는 hub 템플릿 사용\n",
    "retrieval_qa_chat_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"input\"],\n",
    "    template=(\n",
    "        \"Use the following context to answer the question.\\n\\n\"\n",
    "        \"Context:\\n{context}\\n\\n\"\n",
    "        \"Question: {input}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-truth",
   "metadata": {},
   "source": [
    "### LangChain Expression Language (LCEL) 형식으로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mysterious-myrtle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous agents are systems that can perform tasks independently, making decisions and taking actions based on their programming and environment. They are equipped with the ability to plan ahead, decompose complex tasks into manageable steps, and reflect on their past actions to improve their performance over time. In the context of the provided text, autonomous agents are powered by Large Language Models (LLMs) and are capable of simulating human-like behavior in virtual environments, such as the Generative Agents experiment.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vectorstore.as_retriever() | format_docs,\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | retrieval_qa_chat_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = qa_chain.invoke(\"What are autonomous agents?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-georgia",
   "metadata": {},
   "source": [
    "위 구현 방식은 **포괄적(verbose)**임.\n",
    "\n",
    "이 composition logic을 \n",
    "1. 커스터마이즈하고 helper function으로 감싸거나\n",
    "2. 상위 수준의 `create_retrieval_chain`과 `create_stuff_documents_chain helper` 함수를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "documented-secondary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous agents are systems that can perform tasks independently, making decisions and taking actions based on their programming and environment. They are equipped with the ability to plan ahead, decompose complex tasks into manageable steps, and reflect on their past actions to improve their performance over time. In the context of the provided text, autonomous agents are powered by Large Language Models (LLMs) and are capable of simulating human-like behavior in virtual environments, such as the Generative Agents experiment.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "rag_chain = create_retrieval_chain(vectorstore.as_retriever(), combine_docs_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What are autonomous agents?\"})\n",
    "\n",
    "# response는 json object형태로 반환되며, 그 중 'answer'가 LLM의 답변이다.\n",
    "print(response['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_dataloaders",
   "language": "python",
   "name": "rag_dataloaders"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
